# LangChain.js + Ollama Intelligent Q&A System

A local intelligent question-answering system based on LangChain.js and Ollama, supporting document retrieval and natural language Q&A.

## ğŸš€ Features

-   **Local Deployment**: Uses Ollama local large models, no internet required
-   **Document Retrieval**: Supports text chunking and vectorized retrieval
-   **Modern Architecture**: Built with LangChain Expression Language (LCEL)
-   **Chinese Support**: Fully supports Chinese Q&A
-   **Flexible Configuration**: Customizable retrieval parameters and model settings
-   **Interactive CLI**: Command-line interface with conversation history and commands

## ğŸ“‹ System Requirements

-   Node.js 18+
-   Ollama installed and running
-   At least 4GB available memory (for running large models)

## ğŸ› ï¸ Installation Steps

### 1. Clone the project

```bash
git clone <your-repo-url>
cd testing-langchainjs
```

### 2. Install dependencies

```bash
npm install
```

### 3. Install Ollama models

```bash
# Install large language model
ollama pull llama3

# Install embedding model
ollama pull mxbai-embed-large
```

### 4. Start Ollama service

```bash
ollama serve
```

## ğŸ¯ Usage

### Basic usage

```bash
# Run the main program
node src/index.js
```

### Interactive mode (Recommended)

```bash
# Run the advanced interactive system
node src/interactive-chat.js
```

### Custom questions

Modify the question in `src/index.js`:

```js
const result = await chain.invoke('Your question');
```

### Add new documents

1. Place documents in the `src/sources/txt/` directory
2. Modify the file path in `src/index.js`:

```js
const loader = new TextLoader('./src/sources/txt/your-file.txt');
```

## ğŸ® Interactive Commands

When using the interactive mode (`src/interactive-chat.js`), you can use these commands:

-   `/help` - Show available commands
-   `/history` - Show conversation history
-   `/clear` - Clear conversation history
-   `/status` - Show system status
-   `/quit` or `/exit` - Exit the program

### Example Interactive Session

```
ğŸ¤– Interactive Q&A System
ğŸ’¡ Type your question or use commands (type /help for commands)
ğŸ“š Available context: Situ Yongcong's profile
============================================================

â“ Your question: What is Situ Yongcong's expertise?

ğŸ¤” Thinking...

ğŸ¤– Answer:
æ ¹æ®æ–‡æ¡£ï¼Œå¸å¾’æ°¸èªçš„æŠ€æœ¯ä¸“é•¿åŒ…æ‹¬ï¼š

1. ç³»ç»Ÿè¿ç»´ï¼ˆSystem Operationsï¼‰
2. å‰ç«¯å¼€å‘ï¼ˆFront-end Developmentï¼‰
3. QNAP NAS è°ƒä¼˜
4. è‡ªåŠ¨åŒ–è„šæœ¬å¼€å‘
5. ç¡¬ä»¶è¯Šæ–­

ä»–ä¸“æ³¨äºæŠ€æœ¯å®è·µï¼Œæ“…é•¿ä»æ­å»ºåšå®¢åˆ°æ•…éšœè¯Šæ–­ï¼Œå†åˆ°è‡ªåŠ¨åŒ–è„šæœ¬çš„å„ç§æŠ€æœ¯é¢†åŸŸã€‚

--- å–µ

â“ Your question: /history

ğŸ“ Conversation History:
==================================================

1. Q: What is Situ Yongcong's expertise?
   A: æ ¹æ®æ–‡æ¡£ï¼Œå¸å¾’æ°¸èªçš„æŠ€æœ¯ä¸“é•¿åŒ…æ‹¬ï¼š1. ç³»ç»Ÿè¿ç»´ï¼ˆSystem Operationsï¼‰2. å‰ç«¯å¼€å‘ï¼ˆFront-end Developmentï¼‰...
   Time: 2025/7/12 15:30:45
```

## ğŸ“ Project Structure

```
testing-langchainjs/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.js              # Main program entry
â”‚   â”œâ”€â”€ interactive-chat.js   # Advanced interactive system
â”‚   â”œâ”€â”€ text-split.js         # Text splitting module
â”‚   â”œâ”€â”€ embedding.js          # Vectorization module
â”‚   â”œâ”€â”€ retrival-qa-chain.js  # Retrieval Q&A chain
â”‚   â”œâ”€â”€ retrive.js           # Retrieval module
â”‚   â”œâ”€â”€ comparison-demo.js    # Comparison demo
â”‚   â”œâ”€â”€ retrieval-config-demo.js # Retrieval configuration demo
â”‚   â””â”€â”€ sources/
â”‚       â”œâ”€â”€ txt/              # Text documents
â”‚       â””â”€â”€ pdf/              # PDF documents
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ”§ Configuration Options

### Retrieval configuration

```js
vector_store.asRetriever({
    k: 3, // Return top 3 documents
    searchType: 'similarity', // Similarity search
});
```

### Text splitting configuration

```js
const textSplitter = new CharacterTextSplitter({
    chunkSize: 500, // Maximum characters per chunk
    chunkOverlap: 50, // Overlap between chunks
});
```

### Model configuration

```js
const llm = new Ollama({
    model: 'llama3',
    baseUrl: 'http://127.0.0.1:11434',
    temperature: 0,
});
```

## ğŸ†š LCEL vs Traditional Approach

### LCEL approach (Recommended)

```js
const chain = RunnableSequence.from([
    {
        docs: vector_store.asRetriever({k: 3}),
        question: new RunnablePassthrough(),
    },
    PromptTemplate.fromTemplate('Answer: {question}\nContext: {docs}'),
    llm,
    new StringOutputParser(),
]);
```

### Traditional approach

```js
const combineChain = await createStuffDocumentsChain({llm, prompt});
const retrievalChain = await createRetrievalChain({
    combineDocsChain: combineChain,
    retriever: vector_store.asRetriever(3),
});
```

## ğŸ¨ Advanced Features

### 1. Maximum Marginal Relevance (MMR) Search

```js
vector_store.asRetriever({
    k: 3,
    searchType: 'mmr',
    fetchK: 5,
    lambda: 0.5,
});
```

### 2. Similarity threshold filtering

```js
vector_store.asRetriever({
    k: 3,
    scoreThreshold: 0.7,
});
```

### 3. Custom prompt templates

```js
const prompt = PromptTemplate.fromTemplate(`
Answer the following question based on the documents: {question}

Relevant documents:
{docs}

Please answer in Chinese and add "--- Meow" at the end of your answer.
`);
```

### 4. Conversation history

The interactive system maintains conversation history and includes it in context for better continuity.

## ğŸ› Common Issues

### Q: Encounter "model not found" error?

A: Make sure the corresponding Ollama model is installed:

```bash
ollama list  # View installed models
ollama pull llama3  # Install missing model
```

### Q: Encounter "invalid input type" error?

A: Make sure to use `fromTexts` instead of `fromDocuments`:

```js
// Correct
MemoryVectorStore.fromTexts(texts, metadata, embeddings);

// Wrong
MemoryVectorStore.fromDocuments(documents, embeddings);
```

### Q: Slow retrieval speed?

A: You can adjust retrieval parameters:

```js
vector_store.asRetriever({
    k: 2, // Reduce number of returned documents
    scoreThreshold: 0.8, // Increase similarity threshold
});
```

### Q: Poor answer quality?

A: Try using MMR search:

```js
vector_store.asRetriever({
    searchType: 'mmr',
    fetchK: 5,
    lambda: 0.5,
});
```

## ğŸ“Š Performance Optimization Tips

1. **Document chunking**: Reasonably set `chunkSize` and `chunkOverlap`
2. **Retrieval quantity**: Adjust `k` value according to needs
3. **Model selection**: Choose appropriate models based on hardware configuration
4. **Caching mechanism**: Consider adding vector caching to improve performance

## ğŸ¤ Contributing

1. Fork the project
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details

## ğŸ™ Acknowledgments

-   [LangChain.js](https://js.langchain.com/) - Powerful LLM application framework
-   [Ollama](https://ollama.ai/) - Local large model runtime environment
-   [Chroma](https://www.trychroma.com/) - Vector database

## ğŸ“ Contact

-   Author: Situ Yongcong
-   Blog: situ2001.com/blog
-   Email: 2369558390@qq.com

---

â­ If this project helps you, please give it a star!

/*
 * @Author: zhangmingyuan 2369558390@qq.com
 * @Date: 2025-07-12 14:04:32
 * @LastEditors: zhangmingyuan 2369558390@qq.com
 * @LastEditTime: 2025-07-12 14:04:34
 * @FilePath: /testing-langchainjs/src/index.js
 * @Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
 */
import { OllamaEmbeddings } from "@langchain/ollama";
import { Ollama } from "@langchain/ollama";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { PromptTemplate } from "@langchain/core/prompts";
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from '@langchain/core/output_parsers';
import { TextLoader } from 'langchain/document_loaders/fs/text';
import { CharacterTextSplitter } from '@langchain/textsplitters';
import readline from 'readline';

// 1. åµŒå…¥æ¨¡å‹
const embed_model = new OllamaEmbeddings({ model: "mxbai-embed-large" });

// 2. æ„å»ºå‘é‡æ•°æ®åº“
const loader = new TextLoader("./src/sources/txt/situ-profile.txt");
const docs = await loader.load();

const splitter = new CharacterTextSplitter({
  chunkSize: 500,
  chunkOverlap: 50,
});
const chunks = await splitter.splitDocuments(docs);

const vector_store = await MemoryVectorStore.fromTexts(
  chunks.map(chunk => chunk.pageContent),
  chunks.map(chunk => chunk.metadata), // metadata
  embed_model
);

// 3. LLM
const llm = new Ollama({
  model: "llama3",
  baseUrl: "http://127.0.0.1:11434"
});

// 4. Prompt
const SOME_PROMPT = PromptTemplate.fromTemplate(`
Answer the following question: {question}
using the given context:
{docs}
please answer in Chinese, an concat a "\n --- å–µ" in the end of the answer.
`);

// 5. LCEL Chain
const chain = RunnableSequence.from([
  {
    docs: vector_store.asRetriever({
      k: 3, // åªè¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„å‰3ä¸ªæ–‡æ¡£
      searchType: "similarity" // ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢
    }),
    question: new RunnablePassthrough()
  },
  SOME_PROMPT,
  llm,
  new StringOutputParser()
]);

// 6. åˆ›å»ºå‘½ä»¤è¡Œæ¥å£
const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

console.log("ğŸ¤– LangChain.js + Ollama Q&A System");
console.log("ğŸ’¡ Type your question and press Enter (type 'quit' to exit)");
console.log("ğŸ“š Available context: Situ Yongcong's profile");
console.log("=" .repeat(50));

// 7. äº¤äº’å¼é—®ç­”å¾ªç¯
async function askQuestion() {
  rl.question('\nâ“ Your question: ', async (input) => {
    if (input.toLowerCase() === 'quit' || input.toLowerCase() === 'exit') {
      console.log('\nğŸ‘‹ Goodbye!');
      rl.close();
      return;
    }

    if (input.trim() === '') {
      console.log('âš ï¸  Please enter a question.');
      askQuestion();
      return;
    }

    try {
      console.log('\nğŸ¤” Thinking...');
      const result = await chain.invoke(input);
      console.log('\nğŸ¤– Answer:');
      console.log(result);
    } catch (error) {
      console.error('\nâŒ Error:', error.message);
    }

    // ç»§ç»­ä¸‹ä¸€è½®é—®ç­”
    askQuestion();
  });
}

// 8. å¯åŠ¨äº¤äº’å¼é—®ç­”
askQuestion();
